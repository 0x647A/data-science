{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least square\n",
    "\n",
    "Least square is method used in regression analysis in approximate the solutions. In this approach minimalize the sum of the squares of the residuals in every results of equation. So we want the best fitting. \n",
    "\n",
    "In this notebook I will focus on linear model. I use Boston housing dataset. In this task we approximate proce of flat based on features. \n",
    "\n",
    "The dataset have $n$ datapoints $\\boldsymbol x_i$ in a data matrix $\\boldsymbol X$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\boldsymbol {x}_1^T \\\\\n",
    "\\vdots \\\\ \n",
    "\\boldsymbol {x}_n^T \n",
    "\\end{bmatrix} \\boldsymbol {\\theta} = \\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\ \n",
    "y_n \n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Assuming that we have a prediction model $\\hat{y}_i =  f(\\boldsymbol {x}_i) = \\boldsymbol \\theta^T\\boldsymbol{x}_i$.\n",
    "\n",
    "So we have: $\\boldsymbol X\\boldsymbol{\\theta} = \\boldsymbol {y}.$\n",
    "\n",
    "$\\boldsymbol y$ collects all house prices $y_1,\\dotsc, y_n$ of the training set.\n",
    "\n",
    "The goal is to find the best $\\boldsymbol \\theta$ that minimizes the following (least squares) objective:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "&\\sum^n_{i=1}{\\lVert \\boldsymbol \\theta^T\\boldsymbol {x}_i - y_i \\rVert^2} \\\\\n",
    "&= (\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y)^T(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Note that we aim to minimize the squared error between the prediction $\\boldsymbol \\theta^T\\boldsymbol {x}_i$  of the model and the observed data point $y_i$ in the training set. \n",
    "\n",
    "We set the gradient of the least-squares objective to find the optimal parameters $\\boldsymbol 0$:\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "\\nabla_{\\boldsymbol\\theta}(\\boldsymbol X{\\boldsymbol \\theta} - \\boldsymbol y)^T(\\boldsymbol X{\\boldsymbol \\theta} - \\boldsymbol y) &=& \\boldsymbol 0 \\\\\n",
    "\\iff \\nabla_{\\boldsymbol\\theta}(\\boldsymbol {\\theta}^T\\boldsymbol X^T - \\boldsymbol y^T)(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y) &=& \\boldsymbol 0 \\\\\n",
    "\\iff \\nabla_{\\boldsymbol\\theta}(\\boldsymbol {\\theta}^T\\boldsymbol X^T\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y^T\\boldsymbol X\\boldsymbol \\theta - \\boldsymbol \\theta^T\\boldsymbol X^T\\boldsymbol y + \\boldsymbol y^T\\boldsymbol y ) &=& \\boldsymbol 0 \\\\\n",
    "\\iff 2\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta - 2\\boldsymbol X^T\\boldsymbol y &=& \\boldsymbol 0 \\\\\n",
    "\\iff \\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta        &=& \\boldsymbol X^T\\boldsymbol y.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The solution, which gives zero gradient solves the __normal equation__ $$\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta = \\boldsymbol X^T\\boldsymbol y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "boston_X, boston_y = boston.data, boston.target\n",
    "\n",
    "# calculate theta_hat\n",
    "boston_theta_hat = np.linalg.solve(boston_X.T @ boston_X, boston_X.T @ boston_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
